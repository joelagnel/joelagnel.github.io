<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[LinuxInternals.org]]></title>
  <link href="http://www.linuxinternals.org/atom.xml" rel="self"/>
  <link href="http://www.linuxinternals.org/"/>
  <updated>2018-02-11T00:06:51-08:00</updated>
  <id>http://www.linuxinternals.org/</id>
  <author>
    <name><![CDATA[Joel Fernandes]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[USDT for Reliable Userspace Event Tracing]]></title>
    <link href="http://www.linuxinternals.org/blog/2018/02/10/usdt-notes/"/>
    <updated>2018-02-10T23:22:39-08:00</updated>
    <id>http://www.linuxinternals.org/blog/2018/02/10/usdt-notes</id>
    <content type="html"><![CDATA[<p>Userspace program when compiled to native can place tracepoints in them using a USDT (User Statically Defined Tracing), the details of the tracepoints (such as address and arguments) are placed as a note in the ELF binary for other tools to interpret. This at first seems much better than using Uprobes directly on userspace functions, since the latter not only needs symbol information from the binary but is also at the mercy of compiler optimizations and function inlining. In Android we directly write into <code>tracing_mark_write</code> for userspace tracepoints. While this has its advantages (simplicity), it also means that the only way to &ldquo;process&rdquo; the trace information for ALL tracing usecases is by reading back the trace buffer to userspace and processing them offline, thus needing a full user-kernel-user round trip. Further its not possible to easily create triggers on these events (dump the stack or stop tracing when an event fires, for example). Uprobes event tracing on the other hands gets the full benefit that ftrace events do. Also all userspace emitted trace data is a string which has to be parsed and post-processed. USDT seems a nice way to solve some of these problems, since we can then use the user-provided data and create Uprobes at the correct locations, and process these on the fly using BCC without things ever hitting the trace buffer or returning to userspace.</p>

<p>This simple article is based on some notes I made while playing with USDT probes. To build a C program with an SDT probe, you can just include <code>sdt.h</code> and <code>sdt-config.h</code> from the Ubuntu systemtap-sdt-devel package, which works for both arm64 and x86.</p>

<p>C program can be as simple as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#include "sdt.h"
</span><span class='line'>int main() {
</span><span class='line'>  DTRACE_PROBE("test", "probe1");
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>One compiling this, a new <code>.note.stapsdt</code> ELF section is created, which can be read by:
<code>readelf -n &lt;bin-path&gt;</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Displaying notes found in: .note.stapsdt
</span><span class='line'>  Owner                 Data size Description
</span><span class='line'>  stapsdt              0x00000029 NT_STAPSDT (SystemTap probe descriptors)
</span><span class='line'>    Provider: "test"
</span><span class='line'>    Name: "probe1"
</span><span class='line'>    Location: 0x0000000000000664, Base: 0x00000000000006f4, Semaphore: 0x0000000000000000
</span><span class='line'>    Arguments: </span></code></pre></td></tr></table></div></figure>


<p>Here there are no arguments, however we can use <code>DTRACE_PROBE2</code> to pass more, for example for 2 of them:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>int main() {
</span><span class='line'>  int a = 1;
</span><span class='line'>  int b = 2;
</span><span class='line'>
</span><span class='line'>  DTRACE_PROBE2("test", "probe1", a, b);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>The <code>readelf</code> tool now reads:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Displaying notes found in: .note.stapsdt
</span><span class='line'>  Owner                 Data size Description
</span><span class='line'>  stapsdt              0x00000040 NT_STAPSDT (SystemTap probe descriptors)
</span><span class='line'>    Provider: "test"
</span><span class='line'>    Name: "probe1"
</span><span class='line'>    Location: 0x0000000000000672, Base: 0x0000000000000704, Semaphore: 0x0000000000000000
</span><span class='line'>    Arguments: -4@-4(%rbp) -4@-8(%rbp)</span></code></pre></td></tr></table></div></figure>


<p>Notice how the arguments show exactly how to access the parameters at the location. In this case, we know the arguments are on the stack and at momention offsets from the base pointer.</p>

<p>Compiling with <code>-f-omit-frame-pointer</code> shows the following in <code>readelf</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Displaying notes found in: .note.stapsdt
</span><span class='line'>  Owner                 Data size Description
</span><span class='line'>  stapsdt              0x00000040 NT_STAPSDT (SystemTap probe descriptors)
</span><span class='line'>    Provider: "test"
</span><span class='line'>    Name: "probe1"
</span><span class='line'>    Location: 0x0000000000000670, Base: 0x0000000000000704, Semaphore: 0x0000000000000000
</span><span class='line'>    Arguments: -4@-4(%rsp) -4@-8(%rsp)</span></code></pre></td></tr></table></div></figure>


<p>Without the base pointer, the compiler relies on the stack pointer for the arguments. However notice that even though the C program is identical to the previous example, the &ldquo;arguments&rdquo; in the note section of the ELF has changed. This dynamic nature is one of the key reasons why SDT probes are so much better than say using <code>perf probe</code> directly to install Uprobes in the wild. With some help userspace and the compiler, we have more reliable information to access arguments without needing any DWARF debug info.</p>

<p>Compiling with ARM64 gcc shows a slightly different output for the Arguments. Note that that Arguments is just a string which can be post processed by tools to fetch the probe data.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Displaying notes found in: .note.stapsdt
</span><span class='line'>  Owner                 Data size Description
</span><span class='line'>  stapsdt              0x0000003f NT_STAPSDT (SystemTap probe descriptors)
</span><span class='line'>    Provider: "test"
</span><span class='line'>    Name: "probe1"
</span><span class='line'>    Location: 0x000000000000077c, Base: 0x0000000000000820, Semaphore: 0x0000000000000000
</span><span class='line'>    Arguments: -4@[sp, 12] -4@[sp, 8]</span></code></pre></td></tr></table></div></figure>


<p>USDT limitations as I see it:</p>

<ul>
<li><p>No information about types is stored. This is kind of sad, since now inorder to know what do with the values, one needs more information. These tracepoints were used with DTrace and SystemTap and turns out the scripts that probe these tracepoints are where the type information is stored or assumed. Uprobes tracer supports &ldquo;string&rdquo; but without knowing that the USDT is a string value, there&rsquo;s no way a Uprobe can be created on it, since all the stap note tells us is that there&rsquo;s a pointer there (who knows if its a 64-bit integer or a character pointer, for example).</p></li>
<li><p>Argument names are also not stored. This means arguments have to be in the same order in the debug script as they are in the program being debugged.</p></li>
</ul>


<p>It seems with a little bit of work, both these things can be added. Does that warrant a new section or can the stapsdt section be augment without causing breakage of existing tools? I don&rsquo;t know yet.</p>

<h2>SDT parsing logic</h2>

<p>BCC has a USDT parser written to extract probes from the ELF. Read <code>parse_stapsdt_note</code> in <code>src/cc/bcc_elf.c</code> in the BCC tree for details.</p>

<h2>Dynamic programming languages</h2>

<p>Programs that are interpretted can&rsquo;t provide this information ahead of time. The <a href="https://github.com/sthima/libstapsdt#how-it-works">libstapsdt</a> tries to solve this by <a href="https://github.com/sthima/libstapsdt/blob/6045277309ff0425322bed5e71393ce5c8fa1344/src/libstapsdt.c#L89">creating a shared library on the fly</a> and linking to it from the dynamic program. This seems a bit fragile but appears to have users. There are wrappers in Python and Nodejs. <a href="https://medium.com/sthima-insights/we-just-got-a-new-super-power-runtime-usdt-comes-to-linux-814dc47e909f">Check this article</a> for more details.</p>

<p>Open question I have:
* Do any existing Linux tools handle USDT strings? Uprobe tracer does support strings, so the infrastructure seems to be there. I didn&rsquo;t see any hints of this in <a href="src/cc/usdt/usdt_args.cc">BCC</a>. Neither does <a href="https://github.com/sthima/libstapsdt/blob/6045277309ff0425322bed5e71393ce5c8fa1344/src/libstapsdt.h#L14">libstapsdt</a> seem to have this.</p>

<h2>Other ideas</h2>

<ul>
<li><p>Creating Uprobes on the fly when a process is loaded: Ideally speaking, if the ELF note section had all the information that the kernel needed, then we could create the Uprobe events for the uprobe trace events at load time and keep them disabled without needing userspace to do anything else. This seems crude at first, but in the &ldquo;default&rdquo; case, it would still have almost no-overhead. This does mean that all the information Uprobe tracing needs will have to be stored in the note section. The other nice thing about this is, you no longer need to know the PID of all processes with USDTs in them.</p></li>
<li><p>For dynamic languages, <code>libstapsdt</code> seems great, but it feels a bit hackish since it creates a temporary file for the stub. Perhaps uprobes can be created after the temporary file is dlopen&#8217;ed and then the file can be unlinked if it hasn&rsquo;t been already, so that there aren&rsquo;t any more references to the temporary file in the file system. Such a temporary file could also probably be in a RAM based file system.</p></li>
</ul>


<h2>References</h2>

<ol>
<li><a href="http://www.brendangregg.com/blog/2015-07-03/hacking-linux-usdt-ftrace.html">Brendan Gregg&rsquo;s USDT ftrace page</a></li>
<li><a href="https://www.kernel.org/doc/Documentation/trace/uprobetracer.txt">Uprobe tracer in the kernel</a></li>
<li><a href="https://medium.com/sthima-insights/we-just-got-a-new-super-power-runtime-usdt-comes-to-linux-814dc47e909f">USDT for dynamic languages</a></li>
<li><a href="https://dzone.com/articles/next-generation-linux-tracing">Sasha Goldstein&rsquo;s &ldquo;Next Generation Linux Tracing With BPF&rdquo; article</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[BPFd- Running BCC Tools Remotely Across Systems and Architectures]]></title>
    <link href="http://www.linuxinternals.org/blog/2018/01/08/bpfd-bcc/"/>
    <updated>2018-01-08T00:00:00-08:00</updated>
    <id>http://www.linuxinternals.org/blog/2018/01/08/bpfd-bcc</id>
    <content type="html"><![CDATA[<p>This article (with some edits) also <a href="https://lwn.net/Articles/744522/">appeared on LWN</a>.</p>

<h2>Introduction</h2>

<p><a href="https://github.com/iovisor/bcc/blob/master/README.md">BCC (BPF Compiler Collection)</a> is a toolkit and a suite of kernel
tracing tools that allow systems engineers to efficiently and safely get a deep understanding into the inner workings of
a Linux system. Because they can&rsquo;t crash the kernel, they are safer than kernel modules and can be run in production
environments. <a href="http://www.brendangregg.com/ebpf.html">Brendan Gregg has written</a> nice tools and given talks showing the
full power of eBPF based tools. Unfortunately, BCC has no support for a cross-development workflow. I define
&ldquo;cross-development&rdquo; as a development workflow in which the development machine and the target machine running the
developed code are different. Cross-development is very typical among Embedded System kernel developers who often
develop on a powerful x86 host and then flash and test their code on SoCs (System on Chips) based on the ARM
architecture. Not having a cross-development flow gives rise to several complications, lets go over them and discuss a
solution called BPFd that cleverly addresses this issue.</p>

<p>In the Android kernel team, we work mostly on ARM64 systems, since most Android devices are on this architecture. BCC
tools support on ARM64 systems has stayed broken for years. One of the reasons for this difficulty is with ARM64 inline
assembler statements. Unavoidably, kernel header includes in BCC tools result in inclusion of asm headers which in the
case of ARM64 has the potential of spewing inline asm ARM64 instructions <a href="https://www.mail-archive.com/iovisor-dev@lists.iovisor.org/msg00299.html">causing major
pains</a>  to LLVM&rsquo;s BPF backend. Recently this
issue got fixed by BPF inline asm support (these
<a href="https://github.com/llvm-mirror/llvm/commit/2865ab6996164e7854d55c9e21c065fad7c26569">LLVM</a> <a href="https://github.com/llvm-mirror/llvm/commit/a6b7d22c2e64f44e6c74ad7e5ce5670f5ae72da3">commits</a>) and <a href="(https://github.com/iovisor/bcc/issues/1202">folks could finally run BCC
tools on arm64</a>), but..</p>

<p>In order for BCC tools to work at all, they need kernel sources. This is because most tools need to register callbacks
on the ever-changing kernel API in order to get their data. Such callbacks are registered using the
<a href="https://lwn.net/Articles/132196/">kprobe</a> infrastructure. When a BCC tool is run, BCC switches its current directory
into the kernel source directory before compilation starts, and compiles the C program that embodies the BCC tool&rsquo;s
logic. The C program is free to include kernel headers for <code>kprobes</code> to work and to use kernel data structures.</p>

<p>Even if one were not to use <code>kprobes</code>, BCC also implicity adds a common <code>helpers.h</code> include directive  whenever an eBPF
C program is being compiled, found in <code>src/cc/export/helpers.h</code> in the BCC sources. This <code>helpers.h</code> header uses the
<code>LINUX_VERSION_CODE</code> macro to create a &ldquo;version&rdquo; section in the compiled output. <code>LINUX_VERSION_CODE</code> is available only
in the specific kernel&rsquo;s sources being targeted and is used during eBPF program loading to make sure the BPF program is
being loaded into a kernel with the right version. As you can see, kernel sources quickly become mandatory for compiling
eBPF programs.</p>

<p>In some sense this build process is similar to how external kernel modules are built. Kernel sources are large in size
and often can take up a large amount of space on the system being debugged. They can also get out of sync, which may
make the tools misbehave.</p>

<p>The other issue is Clang and LLVM libraries need to be available on the target being traced. This is because the tools
compile the needed BPF bytecode which are then loaded into the kernel. These libraries take up a lot space. It seems
overkill that you need a full-blown compiler infrastructure on a system when the BPF code can be compiled elsewhere and
maybe even compiled just once. Further, these libraries need to be cross-compiled to run on the architecture you&rsquo;re
tracing. That&rsquo;s possible, but why would anyone want to do that if they didn&rsquo;t need to? Cross-compiling compiler
toolchains can be tedious and stressful.</p>

<h2>BPFd: A daemon for running eBPF BCC tools across systems</h2>

<p><a href="https://github.com/joelagnel/bpfd">Sources for BPFd can be downloaded here</a>.</p>

<p>Instead of loading up all the tools, compiler infrastructure and kernel sources onto the remote targets being traced and
running BCC that way, I decided to write a proxy program named BPFd that receives commands and performs them on behalf
of whoever is requesting them. All the heavily lifting (compilation, parsing of user input, parsing of the hash maps,
presentation of results etc) is done by BCC tools on the host machine, with BPFd running on the target and being the
interface to the target kernel. BPFd encapsulates all the needs of BCC and performs them &ndash; this includes loading a BPF
program, creating, deleting and looking up maps, attaching a eBPF program to a kprobe, polling for new data that the
eBPF program may have written into a perf buffer, etc. If it&rsquo;s woken up because the perf buffer contains new data, it&rsquo;ll
inform BCC tools on the host about it, or it can return map data whenever requested, which may contain information
updated by the target eBPF program.</p>

<h3>Simple design</h3>

<p>Before this work, the BCC tools architecture was as follows:
<img src="images/bcc-arch.png" alt="BCC architecture" /></p>

<p>BPFd based invocations partition this, thus making it possible to do cross-development and execution of the tools across
machine and architecture boundaries. For instance, kernel sources that the BCC tools depend on can be on a development
machine, with eBPF code being loaded onto a remote machine. This partioning is illustrated in the following diagram:
<img src="images/bcc-with-bpfd-arch.png" alt="BCC architecture with BPFd" /></p>

<p>The design of BPFd is quite simple, it expects commands on <code>stdin</code> (standard input) and provides the results over
<code>stdout</code> (standard output). Every command a single line always, no matter how big the command is. This allows easy
testing using <code>cat</code>, since one could simply <code>cat</code> a file with commands, and check if BPFd&rsquo;s <code>stdout</code> contain the
expected results. Results from a command, however can be multiple lines.</p>

<p>BPF maps are data structures that a BPF program uses to store data which can be
retrieved at a later time. Maps are represented by file descriptor returned by
the <code>bpf</code> system call once the map has been successfully created.
For example, following is a command to BPFd for creating a BPF hashtable map.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>BPF_CREATE_MAP 1 count 8 40 10240 0</span></code></pre></td></tr></table></div></figure>


<p>And the result from BPFd is:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bpf_create_map: ret=3</span></code></pre></td></tr></table></div></figure>


<p>Since BPFd is proxying the map creation, the file descriptor (3 in this example) is
 mapped into <code>BPFd's</code> file descriptor table. The file descriptor can be used later to
look up entries that the BPF program in the kernel may have created, or to clear all
entries in the map, as is done by tools that periodically clear the accounting done
by a BPF program.</p>

<p>The <code>BPF_CREATE_MAP</code> command in this example tells BPFd to create a map named
<code>count</code> with map type 1 (<a href="https://github.com/torvalds/linux/blob/master/include/uapi/linux/bpf.h#L101">type 1 is a hashtable
map</a>),
with a key size of 8 bytes and a value size of 40, maximum of 10240 entries and
no special flags. BPFd created a map and identified by file descriptor 3.</p>

<p>With the simple standard input/output design, it&rsquo;s possible to write wrappers around BPFd to handle more advanced
communication methods such as USB or Networking. As a part of my analysis work in the Android kernel team, I am
communicating these commands over the <a href="https://developer.android.com/studio/command-line/adb.html">Android Debug Bridge</a>
which interfaces with the target device over either USB or TCP/IP. I have shared several demos below.</p>

<h3>Changes to the BCC project for working with BPFd</h3>

<p><a href="https://github.com/iovisor/bcc/">BCC</a> needed several changes to be able to talk
to BPFd over a remote connection. All these changes <a href="https://github.com/joelagnel/bcc/tree/bcc-bpfd">are available
here</a> and will be pushed
upstream soon.</p>

<p>Following are all the BCC modifications that have been made:</p>

<h4>Support for remote communication with BPFd such as over the network</h4>

<p>A new <code>remotes</code> module has been added to BCC tools with an abstraction that
different remote types, such as networking or USB must implement. This keeps
code duplication to a minimum. By implementing <a href="https://github.com/joelagnel/bcc/blob/bcc-bpfd/src/python/bcc/remote/base.py">the functions
needed</a>
for a remote, a new communication method can be easily added. Currently an
<code>adb</code> remote and a <code>process</code> remote are provided. The <code>adb</code> remote is for
communication with the target device over USB or TCP/IP using the <a href="https://developer.android.com/studio/command-line/adb.html">Android
Debug Bridge</a>. The
<code>process</code> remote is probably useful just for local testing. With the <code>process</code> remote,
BPFd is forked on the same machine running BCC and communicates with it over
<code>stdin</code> and <code>stdout</code>.</p>

<h4>Changes to BCC to send commands to the remote BPFd</h4>

<p><a href="https://github.com/iovisor/bcc/blob/master/src/cc/libbpf.c">libbpf.c</a> is the
main C file in the BCC project that talks to the kernel for all things BPF.
This is illustrated in the diagram above. Inorder to make BCC perform BPF
operations on the remote machine instead of the local machine, parts of BCC
that make calls to the local <code>libbpf.c</code> are now instead channeled to the remote
BPFd on the target. BPFd on the target then perform the commands on behalf of
BCC running locally, by calling into its copy of <code>libbpf.c</code>.</p>

<p>One of the tricky parts to making this work is, not only calls to <code>libbpf.c</code>
but certain other paths need to be channeled to the remote machine. For example, to
attach to a tracepoint, BCC needs a list of all available tracepoints on the
system. This list has to be obtained on the remote system, not the local one and is
the exact reason why there exists the <a href="https://github.com/joelagnel/bpfd/blob/master/src/bpfd.c#L421">GET_TRACE_EVENTS</a>
command in BPFd.</p>

<h4>Making the kernel build for correct target processor architecture</h4>

<p>When BCC compiles the C program encapsulated in a BCC tool into eBPF
instructions, it assumes that the eBPF program will run on the same processor
architecture that BCC is running on. This is incorrect especially when building
the eBPF program for a different target.</p>

<p>Some time ago, before I started this project, I <a href="https://patchwork.kernel.org/patch/9961801/">changed
this</a> when building the in-kernel
eBPF samples (which are simple standalone samples and unrelated to BCC). Now, I have had to <a href="https://github.com/joelagnel/bcc/commit/2a2f9d41c336d8aa058338ae536bd93d31dbb1ef">make a similar
change</a>
to BCC so that it compiles the C program correctly for the target architecture.</p>

<h3>Installation</h3>

<p>Try it out for yourself! Follow the <a href="https://github.com/joelagnel/bpfd/blob/master/INSTALL.md#diy">Detailed</a> or
<a href="https://github.com/joelagnel/bpfd/blob/master/INSTALL.md">Simple</a> instructions. Also, apply this <a href="https://raw.githubusercontent.com/joelagnel/bpfd/master/patches/kernel/0001-bpf-stackmap-Implement-bpf_get_next_key.patch">kernel
patch</a>
to make it faster to run tools like offcputime. I am submitting this patch to LKML as we speak.</p>

<h3>BPF Demos: examples of BCC tools running on Android</h3>

<h4>Running filetop</h4>

<p><code>filetop</code> is a BCC tool which shows you all read/write I/O operations with a similar experience to the <code>top</code> tool.
It refreshes every few seconds, giving you a live view of these operations.
Goto your bcc directory and set the environment variables needed. For Android running on Hikey960, I run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>joel@ubuntu:~/bcc# source arm64-adb.rc</span></code></pre></td></tr></table></div></figure>


<p>which basically sets the following environment variables:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  export ARCH=arm64
</span><span class='line'>  export BCC_KERNEL_SOURCE=/home/joel/sdb/hikey-kernel/
</span><span class='line'>  export BCC_REMOTE=adb</span></code></pre></td></tr></table></div></figure>


<p>You could also use the convenient bcc-set script provided in BPFd sources to set these environment variables for you.
Check <a href="https://github.com/joelagnel/bpfd/blob/master/INSTALL.md">INSTALL.md</a> file in BPFd sources for more information.</p>

<p>Next I start <code>filetop</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>joel@ubuntu:~/bcc# ./tools/filetop.py 5</span></code></pre></td></tr></table></div></figure>


<p>This tells the tool to monitor file I/O every 5 seconds.</p>

<p>While <code>filetop</code> is running, I start the stock email app in Android and the output looks like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  Tracing... Output every 5 secs. Hit Ctrl-C to end
</span><span class='line'>  13:29:25 loadavg: 0.33 0.23 0.15 2/446 2931
</span><span class='line'> 
</span><span class='line'>  TID    COMM             READS  WRITES R_Kb    W_Kb    T FILE
</span><span class='line'>  3787   Binder:2985_8    44     0      140     0       R profile.db
</span><span class='line'>  3792   m.android.email  89     0      130     0       R Email.apk
</span><span class='line'>  3813   AsyncTask #3     29     0      48      0       R EmailProvider.db
</span><span class='line'>  3808   SharedPreferenc  1      0      16      0       R AndroidMail.Main.xml
</span><span class='line'>  3792   m.android.email  2      0      16      0       R deviceName
</span><span class='line'>  3815   SharedPreferenc  1      0      16      0       R MailAppProvider.xml
</span><span class='line'>  3813   AsyncTask #3     8      0      12      0       R EmailProviderBody.db
</span><span class='line'>  2434   WifiService      4      0      4       0       R iface_stat_fmt
</span><span class='line'>  3792   m.android.email  66     0      2       0       R framework-res.apk</span></code></pre></td></tr></table></div></figure>


<p>Notice the Email.apk being read by Android to load the email application, and then various other reads happening related
to the email app. Finally, WifiService continously reads iface_state_fmt to get network statistics for Android
accounting.</p>

<h4>Running biosnoop</h4>

<p>Biosnoop is another great tool shows you block level I/O operations (bio) happening on the system along with the latency
and size of the operation. Following is a sample output of running <code>tools/biosnoop.py</code> while doing random things in the
Android system.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  TIME(s)        COMM           PID    DISK    T  SECTOR    BYTES   LAT(ms)
</span><span class='line'>  0.000000000    jbd2/sdd13-8   2135   sdd     W  37414248  28672      1.90
</span><span class='line'>  0.001563000    jbd2/sdd13-8   2135   sdd     W  37414304  4096       0.43
</span><span class='line'>  0.003715000    jbd2/sdd13-8   2135   sdd     R  20648736  4096       1.94
</span><span class='line'>  5.119298000    kworker/u16:1  3848   sdd     W  11968512  8192       1.72
</span><span class='line'>  5.119421000    kworker/u16:1  3848   sdd     W  20357128  4096       1.80
</span><span class='line'>  5.448831000    SettingsProvid 2415   sdd     W  20648752  8192       1.70</span></code></pre></td></tr></table></div></figure>


<h4>Running hardirq</h4>

<p>This tool measures the total time taken by different hardirqs in the systems. Excessive time spent in hardirq can result
in poor real-time performance of the system.</p>

<pre><code>joel@ubuntu:~/bcc# ./tools/hardirqs.py
</code></pre>

<p>Output:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  Tracing hard irq event time... Hit Ctrl-C to end.
</span><span class='line'>  HARDIRQ                    TOTAL_usecs
</span><span class='line'>  wl18xx                             232
</span><span class='line'>  dw-mci                            1066
</span><span class='line'>  e82c0000.mali                     8514
</span><span class='line'>  kirin                             9977
</span><span class='line'>  timer                            22384</span></code></pre></td></tr></table></div></figure>


<h4>Running biotop</h4>

<p>Run biotop while launching the android Gallery app and doing random stuff:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>joel@ubuntu:~/bcc# ./tools/biotop.py</span></code></pre></td></tr></table></div></figure>


<p>Output:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>PID    COMM             D MAJ MIN DISK       I/O  Kbytes  AVGms
</span><span class='line'>4524   droid.gallery3d  R 8   48  ?           33    1744   0.51
</span><span class='line'>2135   jbd2/sdd13-8     W 8   48  ?           15     356   0.32
</span><span class='line'>4313   kworker/u16:4    W 8   48  ?           26     232   1.61
</span><span class='line'>4529   Jit thread pool  R 8   48  ?            4     184   0.27
</span><span class='line'>2135   jbd2/sdd13-8     R 8   48  ?            7      68   2.19
</span><span class='line'>2459   LazyTaskWriterT  W 8   48  ?            3      12   1.77</span></code></pre></td></tr></table></div></figure>


<h3>Open issues as of this writing</h3>

<p>While most issues have been fixed, a few remain. Please check the <a href="https://github.com/joelagnel/bpfd/issues">issue
tracker</a> and contribute patches or help by testing.</p>

<h3>Other usecases for BPFd</h3>

<p>While the main usecase at the moment is easier use of BCC tools on cross-development models, another potential usecase
that&rsquo;s gaining interest is easy loading of a BPF program. The BPFd code can be stored on disk in base64 format and sent
to bpfd using something as simple as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>joel@ubuntu:~/bpfprogs# cat my_bpf_prog.base64 | bpfd</span></code></pre></td></tr></table></div></figure>


<p>In the Android kernel team, we are also expermenting for certain usecases that need eBPF, with loading a program with a
forked BPFd instance, creating maps, and then pinning them for use at a later time once BPFd exits and then kill the
BPFd fork since its done. Creating a separate process (fork/exec of BPFd) and having it load the eBPF program for you
has the distinct advantage that the <a href="https://github.com/torvalds/linux/blob/master/samples/bpf/bpf_load.c#L546">runtime-fixing up map file
descriptors</a> isn&rsquo;t needed in the loaded eBPF machine
instructions. In other words, the eBPF program&rsquo;s instructions can be pre-determined and statically loaded. The reason
for this convience is BPFd starts with the same number of file descriptors each time before the first map is created.</p>

<h3>Conclusion</h3>

<p>Building code for instrumentation on a different machine than the one actually
running the debugging code is beneficial and BPFd makes this possible.
Alternately, one could also write tracing code in their own kernel module on a
development machine, copy it over to a remote target, and do similar
tracing/debugging.  However, this is quite unsafe since kernel modules can
crash the kernel. On the other hand, eBPF programs are verified before they&rsquo;re
run and are guaranteed to be safe when loaded into the kernel, unlike kernel
modules.  Furthermore, the BCC project offers great support for parsing the
output of maps, processing them and presenting results all using the friendly
Python programming language. BCC tools are quite promising and could be the
future for easier and safer deep tracing endeavours. BPFd can hopefully make it
even more easier to run these tools for folks such as Embedded system and Android
developers who typically compile their kernels on their local machine and run
them on a non-local target machine.</p>

<p>If you have any questions, feel to <a href="http://www.linuxinternals.org/joel/">reach out</a> to me or drop me a note in the
comments section.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ARMv8: Flamegraph and NMI Support]]></title>
    <link href="http://www.linuxinternals.org/blog/2016/12/31/nmi-perf-armv8/"/>
    <updated>2016-12-31T21:29:26-08:00</updated>
    <id>http://www.linuxinternals.org/blog/2016/12/31/nmi-perf-armv8</id>
    <content type="html"><![CDATA[<p>Non-maskable interrupts (NMI) is a really useful feature for debugging, that hardware can provide. Unfortunately ARM doesn&rsquo;t provide an out-of-the-box NMI interrupt mechanism. This post shows a flamegraph issue due to missing NMI support, and the upstream work being done to simulate NMI in ARMv8.</p>

<p>Some great Linux kernel features that rely on NMI to work properly are:</p>

<ul>
<li><p>Backtrace from all CPUs: A number of places in the kernel rely on dumping the stacks of all CPUs at the time of a failure to determine what was going on. Some of them are <a href="http://lxr.free-electrons.com/source/kernel/hung_task.c">Hung Task detection</a>, <a href="http://lxr.free-electrons.com/source/Documentation/lockup-watchdogs.txt">Hard/soft lockup detector</a> and spinlock debugging code.</p></li>
<li><p>Perf profiling and flamegraphs: To be able to profile code that runs in interrupt handlers, or in sections of code that disable interrupts, Perf relies on NMI support in the architecture. <a href="http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">flamegraphs</a> are a great visual representation of perf profile output. Below is a flamegraph I generated from perf profile output, that shows just what happens on an architecture like ARMv8 with missing NMI support. Perf is using maskable-interrupts on this platform for profiling:</p></li>
</ul>


<p><img src="http://www.linuxinternals.org/images/nmi/flamegraph.png"></p>

<p>As you can see in the area of the flamegraph where the arrow is pointed, a large amount of time is spent in <code>_raw_spin_unlock_irqrestore</code>. It can baffle anyone looking at this data for the first time, and make them think that most of the time is spent in the unlock function. What&rsquo;s actually happenning is because perf is using a maskable interrupt in ARMv8 to do its profiling, any section of code that disables interrupts will not be see in the flamegraph (not be profiled). In other words perf is unable to peek into sections of code where interrupts are disabled. As a result, when interrupts are reenabled during the <code>_raw_spin_unlock_irqrestore</code>, the perf interrupt routine then kicks in and records the large number of samples that elapsed in the interrupt-disable section but falsely accounts it to the _raw_spin_unlock_restore function during which the perf interrupt got a chance to run. Hence the flamegraph anomaly. It is indeed quite sad that ARM still doesn&rsquo;t have a true NMI which perf would love to make use of.</p>

<p>BUT! <a href="https://lkml.org/lkml/2016/8/19/583">Daniel Thompson</a> has been hard at work trying to simulate Non-maskable interrupts on ARMv8. The idea is <a href="http://www.linuxinternals.org/misc/arm-irq-priortization-white-paper.pdf">based on using interrupt priorities</a> and is the subject of the rest of this post.</p>

<h2>NMI Simulation using priorities</h2>

<p>To simulate an NMI, Daniel creates 2 groups of interrupts in his patchset. One group is for all &lsquo;normal&rsquo; interrupts, and the other for non-maskable interrupts (NMI). Non-maskable interrupts are assigned a higher priority than the normal interrupt group. Inorder to &lsquo;mask&rsquo; interrupts in this approach, Daniel replaces the regular interrupt masking scheme in the kernel which happens at the CPU-core level, with setting of the interrupt controller&rsquo;s PMR (priority mask register). When the PMR is set to a certain value, only interrupts which have a higher priority than what&rsquo;s in the PMR will be signaled to a CPU core, all other interrupts will be silenced (masked). By using this technique, it is possible to mask normal interrupts while keeping the NMI unmasked all the time.</p>

<p>Just how does he do this? So, a small primer on interrupts in the ARM world.
ARM uses the GIC <a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dai0176c/ar01s03s01.html">Generic interrupt controller</a> to prioritize and route interrupts to CPU cores. GIC interrupt priorties go from 0 to 255. 0 being highest and 255 being the lowest. By default, the kernel <a href="http://lxr.free-electrons.com/source/include/linux/irqchip/arm-gic.h?v=4.8#L57">assigns priority 0xa0 (192)</a> to all interrupts. He changes this default priority from 0xa0 to 0xc0 (you&rsquo;ll see why).
He then defines what values of PMR would be consider as &ldquo;unmasked&rdquo; vs &ldquo;masked&rdquo;. Masked is 0xb0 and unmasked is 0xf0. This results in the following priorities (greater numbers are lower priority).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>0xf0 (240 decimal)  (11110000 binary) - Interrupts Unmasked (enabled)
</span><span class='line'>0xc0 (192 decimal)  (11000000 binary) - Normal interrupt priority
</span><span class='line'>0xb0 (176 decimal)  (10110000 binary) - Interrupts masked   (disabled)
</span><span class='line'>0x80 (128 decimal)  (10000000 binary) - Non-maskable interrupts</span></code></pre></td></tr></table></div></figure>


<p>In this new scheme, when interrupts are to be masked (disabled), the PMR is set to 0xf0 and when they are unmasked (enabled), the PMR is set to 0xb0. As you can see, setting the PMR to 0xb0 indeed masks normal interrupts, because 0xb0(PMR) &lt; 0xc0(Normal), however non-maskable interrupts still stay unmasked as 0x80(NMI) &lt; 0xb0(PMR). Also notice that inorder to mask/unmask interrupts, all that needs to be done is flip bit 7 in the PMR (0xb0 &ndash;> 0xf0). Daniel largely uses Bit 7 as the mask bit in the patchset.</p>

<h2>Quirk 1: Saving of the PMR context during traps</h2>

<p>Its suggested in the patchset that during traps, the priority value set in the PMR needs to be saved because it may change during traps. To facilitate this, Daniel found a dummy bit in the PSTATE register (PSR). During any exception, Bit 7 of of the PMR is saved into a PSR bit (he calls it the G bit) and restores it on return from the exception. Look at the changes to <code>kernel_entry</code> macro in the set for this code.</p>

<h2>Quirk 2: Ack of masked interrupts</h2>

<p>Note that interrupts are masked before the GIC interrupt controller code can even identify the source of the interrupt. When the GIC code eventually runs, it is tasked with identifying the interrupt source. It does so by reading the <code>IAR</code> register. This read also has the affecting of &ldquo;Acking&rdquo; the interrupt &ndash; in other words, telling the GIC that the kernel has acknowledged the interrupt request for that particular source. Daniel points out that, because the new scheme uses PMR for interrupt masking, its no longer possible to ACK interrupts without first unmasking them (by resetting the PMR) so he temporarily resets PMR, does the <code>IAR</code> read, and restores it. Look for the code in <code>gic_read_iar_common</code> in his patchset to handle this case.</p>

<h2>Open questions I have</h2>

<ul>
<li>Where in the patchset does Daniel mask NMIs once an NMI is in progress, or is this even needed?</li>
</ul>


<h2>Future work</h2>

<p>Daniel has tested his patchset only on the foundation model yet, but it appears that the patch series with modifications should work on the newer Qualcomm chipsets that have the necessary GIC (Generic interrupt controller) access from the core to mess with IRQ priorities. Also, currently Daniel has only implemented CPU backtrace, more work needs to be done for perf support which I&rsquo;ll look into if I can get backtraces working properly on real silicon first.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ftrace Events Mechanism]]></title>
    <link href="http://www.linuxinternals.org/blog/2016/06/18/ftrace-events-mechanism/"/>
    <updated>2016-06-18T22:29:26-07:00</updated>
    <id>http://www.linuxinternals.org/blog/2016/06/18/ftrace-events-mechanism</id>
    <content type="html"><![CDATA[<p>Ftrace events are a mechanism that allows different pieces of code in the kernel to &lsquo;broadcast&rsquo; events of interest. Such as a scheduler context-switch <code>sched_switch</code> for example. In the scheduler core&rsquo;s <code>__schedule</code> function, you&rsquo;ll see something like: <code>trace_sched_switch(preempt, prev, next);</code>
This immediately results in a write to a per-cpu ring buffer storing info about what the previous task was, what the next one is, and whether the switch is happening as a result of kernel preemption (versus happening for other reasons such as a task waiting for I/O completion).</p>

<p>Under the hood, these ftrace events are actually implemented using tracepoints. The terms events are tracepoints appear to be used interchangeably, but it appears one could use a trace point if desired without having to do anything with ftrace. Events on the other hand use ftrace.</p>

<p>Let&rsquo;s discuss a bit about how a tracepoint works. Tracepoints are hooks that are inserted into points of code of interest and call a certain function of your choice (also known as a function probe). Inorder for the tracepoint to do anything, you have to register a function using <code>tracepoint_probe_register</code>. Multiple functions can be registered in a single hook. Once your tracepoint is hit, all functions registered to the tracepoint are executed. Also note that if no function is registered to the tracepoint, then the tracepoint is essentially a NOP with zero-overhead. Actually that&rsquo;s a lie, there is a branch (and some space) overhead only although negligible.</p>

<p>Here is the heart of the code that executes when a tracepoint is hit:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#define __DECLARE_TRACE(name, proto, args, cond, data_proto, data_args) \
</span><span class='line'>        extern struct tracepoint __tracepoint_##name;                   \
</span><span class='line'>        static inline void trace_##name(proto)                          \
</span><span class='line'>        {                                                               \
</span><span class='line'>                if (static_key_false(&__tracepoint_##name.key))         \
</span><span class='line'>                        __DO_TRACE(&__tracepoint_##name,                \
</span><span class='line'>                                TP_PROTO(data_proto),                   \
</span><span class='line'>                                TP_ARGS(data_args),                     \
</span><span class='line'>                                TP_CONDITION(cond),,);                  \
</span><span class='line'>                if (IS_ENABLED(CONFIG_LOCKDEP) && (cond)) {             \
</span><span class='line'>                        rcu_read_lock_sched_notrace();                  \
</span><span class='line'>                        rcu_dereference_sched(__tracepoint_##name.funcs);\
</span><span class='line'>                        rcu_read_unlock_sched_notrace();                \
</span><span class='line'>                }                                                       \
</span><span class='line'>        }                                                   </span></code></pre></td></tr></table></div></figure>


<p>The <code>static_key_false</code> in the above code will evaluate to false if there&rsquo;s no probe registered to the tracepoint.</p>

<p>Digging further, <code>__DO_TRACE</code> does the following in <code>include/linux/tracepoint.h</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#define __DO_TRACE(tp, proto, args, cond, prercu, postrcu)              \
</span><span class='line'>        do {                                                            \
</span><span class='line'>                struct tracepoint_func *it_func_ptr;                    \
</span><span class='line'>                void *it_func;                                          \
</span><span class='line'>                void *__data;                                           \
</span><span class='line'>                                                                        \
</span><span class='line'>                if (!(cond))                                            \
</span><span class='line'>                        return;                                         \
</span><span class='line'>                prercu;                                                 \
</span><span class='line'>                rcu_read_lock_sched_notrace();                          \
</span><span class='line'>                it_func_ptr = rcu_dereference_sched((tp)-&gt;funcs);       \
</span><span class='line'>                if (it_func_ptr) {                                      \
</span><span class='line'>                        do {                                            \
</span><span class='line'>                                it_func = (it_func_ptr)-&gt;func;          \
</span><span class='line'>                                __data = (it_func_ptr)-&gt;data;           \
</span><span class='line'>                                ((void(*)(proto))(it_func))(args);      \
</span><span class='line'>                        } while ((++it_func_ptr)-&gt;func);                \
</span><span class='line'>                }                                                       \
</span><span class='line'>                rcu_read_unlock_sched_notrace();                        \
</span><span class='line'>                postrcu;                                                \
</span><span class='line'>        } while (0)</span></code></pre></td></tr></table></div></figure>


<p>There&rsquo;s a lot going on there, but main part is the loop that goes through all function pointers (probes) that were registered to the tracepoint and calls them one after the other.</p>

<p>Now, here&rsquo;s some secrets. Since all ftrace events are tracepoints under the hood, you can piggy back onto interesting events in your kernel with your own probes. This allows you to write interesting tracers. Infact this is precisely how blktrace works, and also is how SystemTap hooks into ftrace events.
<a href="https://github.com/joelagnel/joel-snips/blob/master/k-patches/cpuhists.diff">Checkout a module I wrote</a> that hooks onto <code>sched_switch</code> to build some histograms. The code there is still buggy but if you mess with it and improve it please share your work.</p>

<p>Now that we know a good amount about tracepoints, ftrace events are easy.</p>

<p>An ftrace event being based on tracepoints, makes full use of it but it has to do more. Ofcourse, it has to write events out to the ring buffer.
When you enable an ftrace event using debug-fs, at that instant the ftrace events framework registers an &ldquo;event probe&rdquo; function at the tracepoint that represents the event. How? Using <code>tracepoint_probe_register</code> just as we discussed.</p>

<p>The code for this is in the file <code>kernel/trace/trace_events.c</code> in function <code>trace_event_reg</code>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>int trace_event_reg(struct trace_event_call *call,
</span><span class='line'>                    enum trace_reg type, void *data)
</span><span class='line'>{
</span><span class='line'>        struct trace_event_file *file = data;
</span><span class='line'>
</span><span class='line'>        WARN_ON(!(call-&gt;flags & TRACE_EVENT_FL_TRACEPOINT));
</span><span class='line'>        switch (type) {
</span><span class='line'>        case TRACE_REG_REGISTER:
</span><span class='line'>                return tracepoint_probe_register(call-&gt;tp,
</span><span class='line'>                                                 call-&gt;class-&gt;probe,
</span><span class='line'>                                                 file);
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>The probe function <code>call-&gt;class-&gt;probe</code> for trace events is defined in the file <code>include/trace/trace_events.h</code> and does the job of writing to the ring buffer. In a nutshell, the code gets a handle into the ring buffer, does assignment of the values to the entry structure and writes it out. There is some magic going on here to accomodate arbitrary number of arguments but I am yet to figure that out.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>static notrace void                                                     \
</span><span class='line'>trace_event_raw_event_##call(void *__data, proto)                       \
</span><span class='line'>{                                                                       \
</span><span class='line'>        struct trace_event_file *trace_file = __data;                   \
</span><span class='line'>        struct trace_event_data_offsets_##call __maybe_unused __data_offsets;\
</span><span class='line'>        struct trace_event_buffer fbuffer;                              \
</span><span class='line'>        struct trace_event_raw_##call *entry;                           \
</span><span class='line'>        int __data_size;                                                \
</span><span class='line'>                                                                        \
</span><span class='line'>        if (trace_trigger_soft_disabled(trace_file))                    \
</span><span class='line'>                return;                                                 \
</span><span class='line'>                                                                        \
</span><span class='line'>        __data_size = trace_event_get_offsets_##call(&__data_offsets, args); \
</span><span class='line'>                                                                        \
</span><span class='line'>        entry = trace_event_buffer_reserve(&fbuffer, trace_file,        \
</span><span class='line'>                                 sizeof(*entry) + __data_size);         \
</span><span class='line'>                                                                        \
</span><span class='line'>        if (!entry)                                                     \
</span><span class='line'>                return;                                                 \
</span><span class='line'>                                                                        \
</span><span class='line'>        tstruct                                                         \
</span><span class='line'>                                                                        \
</span><span class='line'>        { assign; }                                                     \
</span><span class='line'>                                                                        \
</span><span class='line'>        trace_event_buffer_commit(&fbuffer);                            \
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Let me know any comments you have or any other ftrace event behavior you&rsquo;d like explained.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TIF_NEED_RESCHED: Why Is It Needed]]></title>
    <link href="http://www.linuxinternals.org/blog/2016/03/20/tif-need-resched-why-is-it-needed/"/>
    <updated>2016-03-20T01:44:32-07:00</updated>
    <id>http://www.linuxinternals.org/blog/2016/03/20/tif-need-resched-why-is-it-needed</id>
    <content type="html"><![CDATA[<p><code>TIF_NEED_RESCHED</code> is one of the many &ldquo;thread information flags&rdquo; stored along side every task in the Linux Kernel. One of the flags which is vital to the working of preemption is <code>TIF_NEED_RESCHED</code>. Inorder to explain why its important and how it works, I will go over 2 cases where <code>TIF_NEED_RESCHED</code> is used.</p>

<h2>Preemption</h2>

<p>Preemption is the process of forceably grabbing CPU from a user or kernel context and giving it to someone else (user or kernel). It is the means for timesharing a CPU between competing tasks (I will use task as terminology for process).
In Linux, the way it works is a timer interrupt (called the tick) interrupts the task that is running and makes a decision about whether a task or a kernel code path (executing on behalf of a task like in a syscall) is to be preempted. This decision is based on whether the task has been running long-enough and something higher priority woke up and needs CPU now, or is ready to run.</p>

<p>These things happen in <code>scheduler_tick()</code>, the exact path is <em>TIMER HARDWARE INTERRUPT</em> &ndash;> <code>scheduler_tick</code> &ndash;> <code>task_tick_fair</code> &ndash;> <code>entity_tick</code> &ndash;> <code>check_preempt_tick</code>.
<code>entity_tick()</code> updates various run time statistics of the task, and <code>check_preempt_tick()</code> is where TIF_NEED_RESCHED is set.</p>

<p>Here&rsquo;s a small bit of code in <code>check_preempt_tick</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
</span><span class='line'>{
</span><span class='line'>        unsigned long ideal_runtime, delta_exec;
</span><span class='line'>        struct sched_entity *se;
</span><span class='line'>        s64 delta;
</span><span class='line'>
</span><span class='line'>        ideal_runtime = sched_slice(cfs_rq, curr);
</span><span class='line'>        delta_exec = curr-&gt;sum_exec_runtime - curr-&gt;prev_sum_exec_runtime;
</span><span class='line'>        if (delta_exec &gt; ideal_runtime) {
</span><span class='line'>                resched_curr(rq_of(cfs_rq));
</span><span class='line'>                /*
</span><span class='line'>                 * The current task ran long enough, ensure it doesn't get
</span><span class='line'>                 * re-elected due to buddy favours.
</span><span class='line'>                 */
</span><span class='line'>                clear_buddies(cfs_rq, curr);
</span><span class='line'>                return;
</span><span class='line'>        }</span></code></pre></td></tr></table></div></figure>


<p>Here you see a decision is made that the process ran long enough based on its runtime and if so call <code>resched_curr</code>. Turns out <code>resched_curr</code> sets the <code>TIF_NEED_RESCHED</code> for the current task! This informs whoever looks at the flag, that this process should be scheduled out soon.</p>

<p>Even though this flag is set at this point, the task is not going to be preempted yet. This is because preemption happens at specific points such as exit of interrupts. If the flag is set because the timer interrupt (scheduler decided) decided that something of higher priority needs CPU now and sets <code>TIF_NEED_RESCHED</code>, then at the exit of the timer interrupt (interrupt exit path), <code>TIF_NEED_RESCHED</code> is checked, and because it is set &ndash; <code>schedule()</code> is called causing context switch to happen to another process of higher priority, instead of just returning to the existing process that the timer interrupted normally would.
Lets examine where this happens.</p>

<p>For return from interrupt to user-mode:</p>

<p>If the tick interrupt happened user-mode code was running, then in somewhere in the interrupt exit path for x86, this call chain calls schedule <code>ret_from_intr</code> &ndash;> <code>reint_user</code> &ndash;> <code>prepare_exit_to_usermode</code>. Here the need_reched flag is checked, and if true <code>schedule()</code> is called.</p>

<p>For return from interrupt to kernel mode, things are a bit different (skip this para if you think it&rsquo;ll confuse you).</p>

<p>This feature requires kernel preemption to be enabled. The call chain doing the preemption is: <code>ret_from_intr</code> &ndash;> <code>reint_kernel</code> &ndash;> <code>preempt_schedule_irq</code> (see <code>arch/x86/entry/entry_64.S</code>) which calls <code>schedule</code>.
Note that, for return to kernel mode, I see that <code>preempt_schedule_irq</code> calls <code>schedule</code> anyway whether need_resched flag is set or not, this is probably Ok but I am wondering if need_resched should be checked here before <code>schedule</code> is called. Perhaps it would be an optimiziation to avoid unecessarily calling <code>schedule</code>. One reason for not doing so would be, say any other interrupt other than timer tick is returning to the interrupted kernel space, then in these cases for example &ndash; if the timer tick didn&rsquo;t get a chance to run (because all other local interrupts are disabled in Linux until an interrupt finishes, in this case our non-timer interrupt), then we&rsquo;d want the exit path of the non-timer interrupt to behave just like the exit path of the timer tick interrupt would behave, whether need_resched is set or not.</p>

<h2>Critical sections in kernel code where preemption is off</h2>

<p>One nice example of a code path where preemption is off is the <code>mutex_lock</code> path in the kernel. In the kernel, there is an optimization where if a mutex is already locked and not available, but if the lock owner (the task currently holding the lock) is running on another CPU, then the mutex temporarily becomes a spinlock (which means it will spin until it can get the lock) instead of behaving like a mutex (which sleeps until the lock is available). The pseudo code looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mutex_lock() {
</span><span class='line'>  disable_preempt();
</span><span class='line'>  if (lock can't be acquired and the lock holding task is currently running) {
</span><span class='line'>  while (lock_owner_running && !need_resched()) {
</span><span class='line'>      cpu_relax();
</span><span class='line'>  }
</span><span class='line'>  }
</span><span class='line'>  enable_preempt();
</span><span class='line'>  acquire_lock_or_sleep();
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>The lock path does exactly what I described. <code>cpu_relax()</code> is arch specific which is called when the CPU has to do nothing but wait. It gives hints to the CPU that it can put itself into an idle state or use its resources for someone else. For x86, it involves calling the <a href="https://en.wikipedia.org/wiki/HLT">halt instruction</a>.</p>

<p>What I noticed is the Ftrace latency tracer complained about a long delay in the preempt disabled path of mutex_lock for one of my tests, and I made some <a href="http://www.spinics.net/lists/linux-rt-users/msg15022.html">noise</a> about it on the mailing list. Disabling preemption for long periods is generally a bad thing to do because during this duration, no other task can be scheduled on the CPU. However, Steven <a href="http://www.spinics.net/lists/linux-rt-users/msg15025.html">pointed out that</a> for this particular case, since we&rsquo;re checking for need_resched() and breaking out of the loop, we should be Ok. What would happen is, the scheduling timer interrupt (which calls <code>scheduler_tick()</code> I mentioned earlier) comes in and checks if higher priority tasks need CPU, and if they do, it sets <code>TIF_NEED_RESCHED</code>. Once the timer interrupt returns to our tightly spinning loop in mutex_lock, we would break out of the loop having noticed <code>need_resched()</code> and, re-enable preemption as shown in the code above. Thus the long duration of preemption doesn&rsquo;t turn out to be a problem as long tasks that need CPU are prioritized correctly. <code>need_resched()</code> achieved this fairness.</p>

<p>Next time you see <code>if (need_resched())</code> in kernel code, you&rsquo;ll have a better idea why its there :). Let me know your comments if any.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Understanding Critical Section Behavior With Ftrace Output]]></title>
    <link href="http://www.linuxinternals.org/blog/2016/03/12/understanding-critical-section-behavior-with-ftrace-output/"/>
    <updated>2016-03-12T11:08:38-08:00</updated>
    <id>http://www.linuxinternals.org/blog/2016/03/12/understanding-critical-section-behavior-with-ftrace-output</id>
    <content type="html"><![CDATA[<p>The ftrace irqsoff, preempt and preemptirqsoff tracers are amazing tools identify paths in Linux kernel code that block tasks from getting CPU-time. Essentially during these paths, the scheduler has its hands tied behind its back. There are mainly 2 cases where this can happen.</p>

<p>Local Interrupts disabled (masked):
All CPU cores have a bit in a &lsquo;flags&rsquo; or status register, that is used to mask interrupts on that particular CPU. For x86 architectures, its in a <a href="https://en.wikipedia.org/wiki/FLAGS_register">FLAGS register</a> called the <a href="https://en.wikipedia.org/wiki/Interrupt_flag">IF bit</a>. With this bit set, timer interrupts wont be received without which the scheduler wont get a chance to run and schedule something else.</p>

<p>Preemption disabled:
Preemption is often disabled in the kernel when acquiring spinlocks. We went over the mechanics of spinlocks <a href="http://www.linuxinternals.org/blog/2014/05/07/spinlock-implementation-in-linux-kernel/">previously</a>. There are other cases where preepmtion may be disabled as well. With preemption turned off, the scheduler will not preempt an existing task and prevent giving some other task a chance to run.</p>

<p>Both these cases introduce latency to runnable tasks and so they are bad for predictablity and real-timeness. Disabling interrupts is worse than turning off preemption because this also has the added effect of introducing latency to other interrupts.</p>

<p>The tracers I just mentioned are called latency tracers and they trace the total latency and optionally the functions that are executed between when irqs are turned off and on, or when preempt is turned off and on. They are <a href="https://www.kernel.org/doc/Documentation/trace/ftrace.txt">documented well</a> so I wont go over them here.</p>

<p>One thing I want to mention is about the preemptirqsoff tracer, it starts when when <em>either</em> preempt <em>or</em> irqs are turned off, and stops tracing only when <em>both</em> preemption and irqs are turned back on.
This makes the preemptirqsoff tracer excellent in tracing kernel code paths that are blocking a task because regardless of whether preemption or irqs causing the latency, it treats both these cases identical or interchangeable. Borrowing from the amazing documentation, the following example illustrates this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Knowing the locations that have interrupts disabled or
</span><span class='line'>preemption disabled for the longest times is helpful. But
</span><span class='line'>sometimes we would like to know when either preemption and/or
</span><span class='line'>interrupts are disabled.
</span><span class='line'>
</span><span class='line'>Consider the following code:
</span><span class='line'>
</span><span class='line'>    local_irq_disable();
</span><span class='line'>    call_function_with_irqs_off();
</span><span class='line'>    preempt_disable();
</span><span class='line'>    call_function_with_irqs_and_preemption_off();
</span><span class='line'>    local_irq_enable();
</span><span class='line'>    call_function_with_preemption_off();
</span><span class='line'>    preempt_enable();
</span><span class='line'>
</span><span class='line'>The irqsoff tracer will record the total length of
</span><span class='line'>call_function_with_irqs_off() and
</span><span class='line'>call_function_with_irqs_and_preemption_off().
</span><span class='line'>
</span><span class='line'>The preemptoff tracer will record the total length of
</span><span class='line'>call_function_with_irqs_and_preemption_off() and
</span><span class='line'>call_function_with_preemption_off().
</span><span class='line'>
</span><span class='line'>But neither will trace the time that interrupts and/or
</span><span class='line'>preemption is disabled. This total time is the time that we can
</span><span class='line'>not schedule. To record this time, use the preemptirqsoff
</span><span class='line'>tracer.</span></code></pre></td></tr></table></div></figure>


<p>It appears though that function tracing in preempt tracers is a bit broken and I <a href="https://lkml.org/lkml/2016/3/12/24">posted a fix</a> for a bug which prevent tracing functions which were called while interrupts were enabled and preemption is disabled. So in the above example, call_function_with_preemption_off wouldn&rsquo;t be traced. It still tracks the max latency and reports it, but it would omit the tracing of this function.</p>

<p>Coming back to the original topic of understanding critical section kernel behavior, the beautiful ftrace output for latency tracers show you a set of flags on the left beside every function traced which illustrates what is going on with preemption and interrupts in the traced function. Here I will go over a set of trace outputs and show you some interesting things.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tying 2 Voltage Sources/signals Together]]></title>
    <link href="http://www.linuxinternals.org/blog/2015/12/25/tying-2-voltage-sources-slash-signals-together/"/>
    <updated>2015-12-25T12:51:29-08:00</updated>
    <id>http://www.linuxinternals.org/blog/2015/12/25/tying-2-voltage-sources-slash-signals-together</id>
    <content type="html"><![CDATA[<p>Recently I <a href="http://electronics.stackexchange.com/questions/207492/how-are-conflicts-between-voltage-sources-or-signals-resolved/207496">asked</a> a question on StackExchange about what happens when 2 voltage signals are tied together. What&rsquo;s the resultant voltage and what decides this voltage? The whole train of thought started when I was trying to contemplate what happens when you use pull-ups on signals that are not Open Drain.</p>

<p>I create and simulated a Circuit with the same scenario in LTSpice. &ldquo;V&rdquo; is the voltage between the &ldquo;+&rdquo; terminals of V1 and V2 and its shown on the right of the simulation. We will confirm the simulation result by doing some math later.<img src="http://www.linuxinternals.org/images/voltage-conflict/voltage-conflict-1.png"></p>

<p>The question is what is the voltage across the load after hooking them up together. And what do the currents look like? Is there a current flowing between the 2 sources as well (apart from the current flowing to the load) because 5v > 1.8v?
The simulator refuses to do a simulation without your author adding an internal resistance to the voltage sources first. All voltages sources have certain internal resistances, so that&rsquo;s fair. This can be considered analogous to having a voltage signal with a certain resistance along its path which limits its current sourcing (or sinking) capabilities.</p>

<p>So I added 1k resistances internally, normally the resistance of a voltage source is far less than this. AA batteries have just 0.1-0.2ohms.
Now the circuit looks something like this: <img src="http://www.linuxinternals.org/images/voltage-conflict/voltage-conflict-2.png"></p>

<p>One can simply apply <a href="https://en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws#Kirchhoff.27s_current_law_.28KCL.29">Kirchoff&rsquo;s current law</a> to the above circuit, the direction of currents would be as in the circuit. I1 and I2 are the currents flowing through R2 and R1 respectively.</p>

<p>By Kirchoff&rsquo;s law, All the current entering the node labeled V will be equal to the current exiting it even if the currents are not in the actual direction shown above. From this we see:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>I1 = (1.8 - V) / 1k
</span><span class='line'>I2 = (5 - V)   / 1k
</span><span class='line'>I3 = (V - 0)   / 10k
</span><span class='line'>
</span><span class='line'>I3 = I2 + I1
</span><span class='line'>V / 10k  = ((1.8 - V) / 1k) + ((5 - V) / 1k)
</span><span class='line'>V = 3.2381v</span></code></pre></td></tr></table></div></figure>


<p>Fom this we see the voltage at V is somewhere between 5 and 1.8v. Infact, where it is between 5 and 1.8 depends on how strong or weak the resistances associated with the sources are. If the resistances are lower, then the sources have more of an influence and vice versa. An interesting observation is I1 is negative if you plug V=3.2v in the above equation. This means the current for voltage source V2 (the 1.8v voltage source) is actually flowing into it rather than out of it (its being sinked) and so I1 is actually opposite in direction to the picture shown above.</p>

<p>A simpler case is having 2 voltage sources of the exact same voltage values, in this case the circuit would look like:<img src="http://www.linuxinternals.org/images/voltage-conflict/voltage-conflict-3.png"></p>

<p><a href="https://en.wikipedia.org/wiki/Th%C3%A9venin%27s_theorem">Thevenin&rsquo;s theorem</a>  provides an easy simplication into the following, where the equivalent voltage source value is the same but the series resistance is now halved. This results in the following circuit:
<img src="http://www.linuxinternals.org/images/voltage-conflict/voltage-conflict-4.png"></p>

<p>Now you can use the <a href="https://en.wikipedia.org/wiki/Voltage_divider">Voltage divider</a> concept and easily solve this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>V = V2 * (R2 / (R1 + R2) )
</span><span class='line'>  = 1.8v * ( 10k / (10k + 0.5k) )
</span><span class='line'>  = 1.7142v</span></code></pre></td></tr></table></div></figure>


<p>As you would notice, the 1k resistance dropped around 0.085v of voltage before getting to the 10k load.
Thanks for reading. Please leave your comments or inputs below.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MicroSD Card Remote Switch]]></title>
    <link href="http://www.linuxinternals.org/blog/2014/06/04/a-microsd-card-remote-switcher/"/>
    <updated>2014-06-04T04:12:55-07:00</updated>
    <id>http://www.linuxinternals.org/blog/2014/06/04/a-microsd-card-remote-switcher</id>
    <content type="html"><![CDATA[<p>Recently, I&rsquo;ve been wanting to remotely be able to program a MicroSD card with a new bootloader or filesystem <em>without</em> removing the card from its embedded target board (such as a Beaglebone or Pandaboard). Due to the lack of any such existing tools, I decided to design my own board. Finally have got it working, below are some pictures and a screencast demo video of the switcher in action! I sprinkled some power and status LED to show the user what&rsquo;s going on.</p>

<p>The base board requires two <a href="https://www.sparkfun.com/products/9419">SparkFun MicroSD sniffers</a>. The card cage on the sniffer is unused for my purposes. The switcher is controlled through an <a href="https://www.sparkfun.com/products/9717">FTDI cable</a>.
I also <a href="https://github.com/joelagnel/microsd-switch/blob/master/sw/switch.c">wrote up</a> a <code>switch</code> program to control the switcher with libftdi. You just have to pass to it the FTDI&rsquo;s serial number and whether you want to switch to host-mode (for programming the card) or target-mode (for booting the programmed card).
<a href="https://github.com/joelagnel/microsd-switch">Hardware design files</a> are available under a CC-BY-NC-SA 3.0 license.</p>

<p>Screencast<iframe width="100%" height="515" src="http://www.youtube.com/embed/StpIihVQ7oM " frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></p>

<p>Pictures
<img src="https://raw.githubusercontent.com/joelagnel/microsd-switch/master/board-pics/microsd-inaction/photo5.jpg">
<img src="https://raw.githubusercontent.com/joelagnel/microsd-switch/master/board-pics/microsd-inaction/photo2.jpg">
<img src="https://raw.githubusercontent.com/joelagnel/microsd-switch/master/board-pics/front.png"></p>

<p>Hope you enjoyed it, let me know what yout think in the comments:)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux Spinlock Internals]]></title>
    <link href="http://www.linuxinternals.org/blog/2014/05/07/spinlock-implementation-in-linux-kernel/"/>
    <updated>2014-05-07T20:42:45-07:00</updated>
    <id>http://www.linuxinternals.org/blog/2014/05/07/spinlock-implementation-in-linux-kernel</id>
    <content type="html"><![CDATA[<p>This article tries to clarify how spinlocks are implemented in the Linux kernel and how they should be used correctly in the face of preemption and interrupts. The focus of this article will be more on basic concepts than details, as details tend to be forgotten more easily and shouldn&rsquo;t be too hard to look up although attention is paid to it to the extent that it helps understanding.</p>

<p>Fundamentally somewhere in <code>include/linux/spinlock.h</code>, a decision is made on which spinlock header to pull based on whether SMP is enabled or not:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
</span><span class='line'># include &lt;linux/spinlock_api_smp.h&gt;
</span><span class='line'>#else
</span><span class='line'># include &lt;linux/spinlock_api_up.h&gt;
</span><span class='line'>#endif</span></code></pre></td></tr></table></div></figure>


<p>We&rsquo;ll go over how things are implemented in both the SMP (Symmetric Multi-Processor) and UP (Uni-Processor) cases.</p>

<p>For the SMP case, <code>__raw_spin_lock*</code> functions in <code>kernel/locking/spinlock.c</code> are called when one calls some version of a <code>spin_lock</code>.</p>

<p>Following is the definition of the most basic version defined with a macro:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#define BUILD_LOCK_OPS(op, locktype)                                    \
</span><span class='line'>void __lockfunc __raw_##op##_lock(locktype##_t *lock)                   \
</span><span class='line'>{                                                                       \
</span><span class='line'>        for (;;) {                                                      \
</span><span class='line'>                preempt_disable();                                      \
</span><span class='line'>                if (likely(do_raw_##op##_trylock(lock)))                \
</span><span class='line'>                        break;                                          \
</span><span class='line'>                preempt_enable();                                       \
</span><span class='line'>                                                                        \
</span><span class='line'>                if (!(lock)-&gt;break_lock)                                \
</span><span class='line'>                        (lock)-&gt;break_lock = 1;                         \
</span><span class='line'>                while (!raw_##op##_can_lock(lock) && (lock)-&gt;break_lock)\
</span><span class='line'>                        arch_##op##_relax(&lock-&gt;raw_lock);             \
</span><span class='line'>        }                                                               \
</span><span class='line'>        (lock)-&gt;break_lock = 0;                                         \
</span><span class='line'>}                                                                       \</span></code></pre></td></tr></table></div></figure>


<p>The function has several imporant bits. First it disables preemption on line 5, then tries to <em>atomically</em> acquire the spinlock on line 6. If it succeeds it breaks from the <code>for</code> loop on line 7, leaving preemption disabled for the duration of crticial section being protected by the lock. If it didn&rsquo;t succeed in acquiring the lock (maybe some other CPU grabbed the lock already), it enables preemption back and spins till it can acquire the lock keeping <em>preemption enabled during this period</em>. Each time it detects that the lock can&rsquo;t be acquired in the <code>while</code> loop, it calls an architecture specific relax function which has the effect executing some variant of a <code>no-operation</code> instruction that causes the CPU to execute such an instruction efficiently in a lower power state. We&rsquo;ll talk about the <code>break_lock</code> usage in a bit. Soon as it knows the lock is free, say the <code>raw_spin_can_lock(lock)</code> function returned 1, it goes back to the beginning of the <code>for</code> loop and tries to acquire the lock again.</p>

<p>What&rsquo;s important to note here is the reason for keeping preemption enabled (we&rsquo;ll see in a bit that for UP configurations, this is not done). While the kernel is spinning on a lock, other processes shouldn&rsquo;t be kept from preempting the spinning thread. The lock in these cases have been acquired on a <em>different CPU</em> because (assuming bug free code) it&rsquo;s impossible the current CPU which is trying to grab the lock has already acquired it, because preemption is disabled on acquiral. So it makes sense for the spinning kernel thread to be preempted giving others CPU time.
It is also possible that more than one process on the current CPU is trying to acquire the same lock and spinning on it, in this case the kernel gets continuously preempted between the 2 threads fighting for the lock, while some other CPU in the cluster happily holds the lock, hopefully for not too long.</p>

<p>That&rsquo;s where the <code>break_lock</code> element in the lock structure comes in. Its used to signal to the lock-holding processor in the cluster that there is someone else trying to acquire the lock. This can cause the lock to released early by the holder if required.</p>

<p>Now lets see what happens in the UP (Uni-Processor) case.</p>

<p>Believe it or not, it&rsquo;s really this simple:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#define ___LOCK(lock) \
</span><span class='line'>  do { __acquire(lock); (void)(lock); } while (0)
</span><span class='line'>
</span><span class='line'>#define __LOCK(lock) \
</span><span class='line'>  do { preempt_disable(); ___LOCK(lock); } while (0)
</span><span class='line'>
</span><span class='line'>// ....skipped some lines.....
</span><span class='line'>#define _raw_spin_lock(lock)                    __LOCK(lock)</span></code></pre></td></tr></table></div></figure>


<p>All that needs to be done is to disable preemption and acquire the lock. The code really doesn&rsquo;t do anything other than disable preemption. The references to the <code>lock</code> variable are just to suppress compiler warnings as mentioned in comments in the source file.</p>

<p>There&rsquo;s no spinning at all here like the UP case and the reason is simple: in the SMP case, remember we had agreed that while a lock is <em>acquired</em> by a particular CPU (in this case just the 1 CPU), no other process on that CPU should have acquired the lock. How could it have gotten a chance to do so with preemption disabled on that CPU to begin with?</p>

<p>Even if the code is buggy, (say the same process tries to acquires the lock twice), it&rsquo;s still impossible that 2 <em>different processes</em> try to acquire the same lock on a Uni-Processor system considering preemption is disabled on lock acquiral. Following that idea, in the Uni-processor case, since we are running on only 1 CPU, all that needs to be done is to disable preemption, since the fact that we are being allowed to disable preemption to begin with, means that no one else has acquired the lock. Works really well!</p>

<h2>Sharing spinlocks between interrupt and process-context</h2>

<p>It is possible that a critical section needs to be protected by the same lock in both an interrupt and in non-interrupt (process) execution context in the kernel. In this case <code>spin_lock_irqsave</code> and the <code>spin_unlock_irqrestore</code> variants have to be used to protect the critical section. This has the effect of disabling interrupts on the  executing CPU. Imagine what would happen if you just used <code>spin_lock</code> in the process context?</p>

<p>Picture the following:</p>

<ol>
<li>Process context kernel code acquires <em>lock A</em> using <code>spin_lock</code>.</li>
<li>While the lock is held, an interrupt comes in on the same CPU and executes.</li>
<li>Interrupt Service Routing (ISR) tries to acquire <em>lock A</em>, and spins continuously waiting for it.</li>
<li>For the duration of the ISR, the Process context is blocked and never gets a chance to run and free the lock.</li>
<li>Hard lock up condition on the CPU!</li>
</ol>


<p>To prevent this, the process context code needs call <code>spin_lock_irqsave</code> which has the effect of disabling interrupts on that particular CPU along with the regular disabling of preemption we saw earlier <em>before</em> trying to grab the lock.</p>

<p>Note that the ISR can still just call <code>spin_lock</code> instead of <code>spin_lock_irqsave</code> because interrupts are disabled anyway during ISR execution. Often times people use <code>spin_lock_irqsave</code> in an ISR, that&rsquo;s not necessary.</p>

<p>Also note that during the executing of the critical section protected by <code>spin_lock_irqsave</code>, the interrupts are only disabled on the executing CPU. The same interrupt can come in on a different CPU and the ISR will be executed there, but that will not trigger the hard lock condition I talked about, because the process-context code is not blocked and can finish executing the locked critical section and release the lock while the ISR spins on the lock on a different CPU waiting for it. The process context does get a chance to finish and free the lock causing no hard lock up.</p>

<p>Following is what the <code>spin_lock_irqsave</code> code looks like for the SMP case, UP case is similar, look it up. BTW, the only difference here compared to the regular <code>spin_lock</code> I described in the beginning are the <code>local_irq_save</code> and <code>local_irq_restore</code> that accompany the <code>preempt_disable</code> and <code>preempt_enable</code> in the lock code:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#define BUILD_LOCK_OPS(op, locktype)                                    \
</span><span class='line'>unsigned long __lockfunc __raw_##op##_lock_irqsave(locktype##_t *lock)  \
</span><span class='line'>{                                                                       \
</span><span class='line'>        unsigned long flags;                                            \
</span><span class='line'>                                                                        \
</span><span class='line'>        for (;;) {                                                      \
</span><span class='line'>                preempt_disable();                                      \
</span><span class='line'>                local_irq_save(flags);                                  \
</span><span class='line'>                if (likely(do_raw_##op##_trylock(lock)))                \
</span><span class='line'>                        break;                                          \
</span><span class='line'>                local_irq_restore(flags);                               \
</span><span class='line'>                preempt_enable();                                       \
</span><span class='line'>                                                                        \
</span><span class='line'>                if (!(lock)-&gt;break_lock)                                \
</span><span class='line'>                        (lock)-&gt;break_lock = 1;                         \
</span><span class='line'>                while (!raw_##op##_can_lock(lock) && (lock)-&gt;break_lock)\
</span><span class='line'>                        arch_##op##_relax(&lock-&gt;raw_lock);             \
</span><span class='line'>        }                                                               \
</span><span class='line'>        (lock)-&gt;break_lock = 0;                                         \
</span><span class='line'>        return flags;                                                   \
</span><span class='line'>}                                                                       \</span></code></pre></td></tr></table></div></figure>


<p>Hope this post made a few things more clear, there&rsquo;s a lot more to spinlocking. A good reference is <a href="https://www.kernel.org/pub/linux/kernel/people/rusty/kernel-locking/">Rusty&rsquo;s Unreliable Guide To Locking</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Studying Cache-line Sharing Effects on SMP Systems]]></title>
    <link href="http://www.linuxinternals.org/blog/2014/04/24/studying-cache-line-sharing-effects-on-smp-systems/"/>
    <updated>2014-04-24T18:28:24-07:00</updated>
    <id>http://www.linuxinternals.org/blog/2014/04/24/studying-cache-line-sharing-effects-on-smp-systems</id>
    <content type="html"><![CDATA[<p>Having read the chapter on counting and per-CPU counters in <a href="http://www.lulu.com/shop/paul-e-mckenney/is-parallel-programming-hard-and-if-so-what-can-you-do-about-it-first-bw-print-edition/paperback/product-21562459.html">Paul Mckenney&rsquo;s recent book</a>, I thought I would do a small experiment to check how good or bad it would be if those per-CPU counters were close to each other in memory.</p>

<p>Paul talks about using one global shared counter for N threads on N CPUs, and the effects it can have on the cache. Each CPU core&rsquo;s cache in an SMP system will need exclusive rights on a specific cache line of memory, before it can do the write. This means that, at any given time <em>only one</em> CPU can and should do a write to that part of memory.</p>

<p>This is accomplished typically by an invalidate protocol, where each CPU needs to do some inter-processor communication before it can assume it has exclusive access to that cache line, and also read any copies that may still be in some other core&rsquo;s cache and not in main memory. This is an expensive operation that is to be avoided at all costs!</p>

<p>Then Paul goes about saying, OK- let&rsquo;s have a per-thread counter, and have each core increment it independently, and when we need a read out, we would grab a lock and add all of the individual counters together. This works great, assuming each per-thread counter is separated by atleast a cache line. That&rsquo;s guaranteed, when one uses the <code>__thread</code> primitive nicely separating out the memory to reduce cache line sharing effects.</p>

<p>So I decided to flip this around, and have per-thread counters that were closely spaced and do some counting with them. Instead of using <code>__thread</code>, I created an array of counters, each element belonging to some thread. The counters are still separate and not shared, but they may still be in shared cache-line causing the nasty effects we talked about, which I wanted to measure.</p>

<p><a href="https://github.com/joelagnel/smp-experiments/blob/05afb2db4fea1c6c0b4614c180186c10627a341a/cache-sharing.c">My program</a> sets up N counting threads, and assumes its running each of them on a single core on typical multicore system.  Various iterations of per-thread counting is done, with the counters separated by increasing powers of 2 each iteration. After each iteration, I stop all threads, add the per-thread counter values and report the result.</p>

<p>Below are the results of running the program on 3 different SMP systems (2 threads on 2 CPUs, sorry I don&rsquo;t have better multi-core hardware ATM):</p>

<p>Effect of running on a reference ARM dual-core Cortex-A9 system:
<img src="http://www.linuxinternals.org/images/cache-sharing/a9-counts.jpeg"></p>

<p>Notice the jump in through-put once the separation changes from 16 to 32 bytes. That gives us a good idea that the L1 cache line size on Cortex-A9 systems is 32 bytes (8 words). Something the author didn&rsquo;t know for sure in advance (I initially thought it was 64-bytes).</p>

<p>Effect of running on a reference ARM dual-core Cortex-A15 system:
<img src="http://www.linuxinternals.org/images/cache-sharing/a15-counts.jpeg"></p>

<p>L1 Cache-line size of Cortex A-15 is 64 bytes (8 words). Expected jump for a separation of 64 bytes.</p>

<p>Effect of running on a x86-64 i7-3687U dual-core CPU:
<img src="http://www.linuxinternals.org/images/cache-sharing/x86-counts.jpeg">.</p>

<p>L1 Cache-line size of this CPU is 64 bytes too (8 words). Expected jump for a separation of 64 bytes.</p>

<p>This shows your parallel programs need to take care of cache-line alignment to avoid false-sharing effects. Also, doing something like this in your program is an indirect way to find out what the cache-line size is for your CPU, or a direct way to get fired, whichever way you want to look at it. ;)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Design of Fork Followed by Exec in Linux]]></title>
    <link href="http://www.linuxinternals.org/blog/2014/04/22/design-of-fork-followed-by-exec-in-linux/"/>
    <updated>2014-04-22T21:06:11-07:00</updated>
    <id>http://www.linuxinternals.org/blog/2014/04/22/design-of-fork-followed-by-exec-in-linux</id>
    <content type="html"><![CDATA[<p>A lot of folks ask <em>why do you have to do fork and then an exec, to execute a new program?</em> and <em>why can&rsquo;t it be done in one step?</em>, or <em>why does fork <a href="http://man7.org/linux/man-pages/man2/fork.2.html">create a copy-on-writed address space</a>, to only have it thrown away later when you do an exec?</em>. So I decided do a small write up about this topic.</p>

<p>On a separate note, firstly it is important to remember that <code>fork</code> is not used for threading, its primary use is to create a separate process, that is a child of the parent process that called <code>fork</code>.</p>

<p>Normally one might think that doing a <code>fork</code> and separate <code>exec</code> can be combined in one step, and it probably should be. But there are applications of maintaining this separation. Here&rsquo;s a <a href="http://stackoverflow.com/questions/1345320/applications-of-fork-system-call">post that explains</a> why you might need to do just a <code>fork</code> call. Summarizing the post, you may need to setup some initial data and &ldquo;fork&rdquo; a bunch of workers. All these works are supposed to execute in <em>their own</em> address space and share <em>only the initial data</em>. In this case, copy-on-write is extremely useful since the initial data can be shared in physical memory and forking this way would be extremely cheap. The kernel marks all these shared pages as read only, and makes writable copies of shared data when they are written to.</p>

<p>There is a small overhead if <code>fork</code> is followed immediately by an <code>exec</code> system call, since the copy-on-write shared address space is of no use and is thrown away anyway. Combining both the <code>fork</code>, <code>exec</code> in this case might might have some advantages, reducing this overhead.</p>

<h2>Linux Implementation of Copy-on-write (COW) for shared Virtual Memory Areas</h2>

<p>Some of this COW code that executes on a fork can be found in <code>mm/memory.c</code>. There is an is_cow function to detect if a virtual memory area (a region of virtual memory, see <code>/proc/self/maps</code>) is copy-on-write.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='c'><span class='line'><span class="k">static</span> <span class="kr">inline</span> <span class="n">bool</span> <span class="nf">is_cow_mapping</span><span class="p">(</span><span class="n">vm_flags_t</span> <span class="n">flags</span><span class="p">)</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>        <span class="k">return</span> <span class="p">(</span><span class="n">flags</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">VM_SHARED</span> <span class="o">|</span> <span class="n">VM_MAYWRITE</span><span class="p">))</span> <span class="o">==</span> <span class="n">VM_MAYWRITE</span><span class="p">;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>A VMA (Virtual Memory Area) is a contiguous segment of virtual memory belonging to a particular process. Every VMA has a bunch of VM_ flags associated with it. <code>VM_MAYWRITE</code>, relevant to the above code, is used to mark that a mapped region can be changed to writable by mprotect system call. It is possible that a memory region is initially readonly and the user wants to make it writable. <code>VM_MAYWRITE</code> gives that permission. Note that if if the kernel doesn&rsquo;t set <code>VM_MAYWRITE</code>, then the region is automatically not COW because there is no question of writing to it.</p>

<p>When a memory mapping is created via the mmap system call, and if <code>MAP_SHARED</code> is passed in flags, the <code>VM_SHARED</code> bit is set for the VMA and as a result the region is not copy-on-write (The above is_cow_mapping function returns false). By definition, shared memory regions are just that &ndash; shared. So no need copy-on-write. In other words, If the VMA is a shared mapping or is a read only mapping, then it isn&rsquo;t a COW mapping.</p>

<p>Let&rsquo;s take the example of mapping a file using mmap,</p>

<p>By default in the kernel on VMA creation, the VMA flags is set to <code>VM_SHARED = 0</code> and <code>VM_MAYWRITE = 1</code>. Now if mmap is asked it to create a shared mapping of a file by passing it <code>MAP_SHARED</code> flag, for example, that can be shared with other processes that are being forked, then the <code>VM_SHARED</code> bit is set to 1 for that VMA. Additionally if the file is opened in read only mode, then <code>VM_MAYWRITE</code> is set to 1. This has the effect of making is_cow_mapping return false. Ofcourse, the shared mapping doesn&rsquo;t need to be a COW.</p>

<p>On the other hand, if <code>MAP_PRIVATE</code> is passed in the flags to mmap, then <code>VM_SHARED</code> bit is set to 0, and <code>VM_MAYWRITE</code> remains at 1 (regardless of whether the file is read or write opened, since writes will not be carried to the underlying file). This makes is_cow_mapping return true. Indeed, private mappings should be copy-on-write enabled.</p>

<p>You can see the code I&rsquo;m talking about <a href="http://lxr.free-electrons.com/source/mm/mmap.c#L1284">conveniently here</a>.</p>

<p>The important point here is that every mapping is either a COW mapping or not a COW mapping. During the <code>clone</code> system call which is called by <code>fork</code> library call internally, if the <code>CLONE_VM</code> flag is not passed to <code>clone</code> as is the case internally within <code>fork</code>, then all the VMA mappings of the parent process are copied to the child, including the page table entries. In this case, any writes to COW mappings should trigger a copy on write. The main thing to note is the children <em>inherit</em> the COW property of all the copied VMA mappings of its parent and don&rsquo;t need to be explictly marked as COW.</p>

<p>However, If <code>CLONE_VM</code> is passed, then the VMAs are not copied and the memory descriptor of the child and the parent process are the same, in this case the child and parent share the same address space and are thus are threads. <a href="http://lxr.free-electrons.com/source/kernel/fork.c#L879">See for yourself</a>. COW or no COW doesn&rsquo;t matter here.</p>

<p>So here&rsquo;s a question for you, For N <code>clone</code> system calls with <code>!CLONE_VM</code> passed for spawning N threads, we can just create as many VMA copies as we want each time, the COW mappings will take care of themselves. Right? Almost! There&rsquo;s more work&hellip; the <em>physical pages</em> of both the original VMA and the copy VMA have to be marked as read-only. That&rsquo;s the only way Copy-on-write of those will be triggered by the CPU when those pages are written to. Here&rsquo;s the code in <a href="http://lxr.free-electrons.com/source/mm/memory.c#L849">copy_one_pte</a> that sets this up:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='c'><span class='line'>     <span class="cm">/*</span>
</span><span class='line'><span class="cm">      * If it&#39;s a COW mapping, write protect it both</span>
</span><span class='line'><span class="cm">      * in the parent and the child</span>
</span><span class='line'><span class="cm">      */</span>
</span><span class='line'>     <span class="k">if</span> <span class="p">(</span><span class="n">is_cow_mapping</span><span class="p">(</span><span class="n">vm_flags</span><span class="p">))</span> <span class="p">{</span>
</span><span class='line'>             <span class="n">ptep_set_wrprotect</span><span class="p">(</span><span class="n">src_mm</span><span class="p">,</span> <span class="n">addr</span><span class="p">,</span> <span class="n">src_pte</span><span class="p">);</span>
</span><span class='line'>             <span class="n">pte</span> <span class="o">=</span> <span class="n">pte_wrprotect</span><span class="p">(</span><span class="n">pte</span><span class="p">);</span>
</span><span class='line'>     <span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>
There you go, now when the COW memory region is written to, a page fault happens, and the page fault handler knows that the VMA of the faulting page is a COW and that&rsquo;s what triggered the page fault. It can then create a copy of the page and restart the faulting instruction, this time removing the write protection if there aren&rsquo;t any others sharing the VMA. So in short, fork+exec can be expensive if you had done lots of <code>fork</code> calls on a process with a lot of large files. Since all this copying business is wasted on doing a subsequent <code>exec</code> system call.</p>

<p>There is one optimization however, why should you have to do this marking for pages that are not physically present in memory? Those will fault anyway. So the above code is <em>not run</em> if the page is not present, nicely done by checking for <code>!pte_present(pte)</code> to be true before the preceding code.</p>

<p>Please share any comments you may have in the comments section.</p>
]]></content>
  </entry>
  
</feed>
